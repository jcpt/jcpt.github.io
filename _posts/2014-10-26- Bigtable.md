---
layout: post
category: hbase
title: BigTable
tagline: by 袁野
tags: [hbase]
---


Paper:《<a href="http://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a>》


## 1.Overview

BigTable是由Google设计实现的分布式非关系数据库，可管理PB级别的数据，包含上千台服务器组成的集群。Bigtable提供了灵活、高性能的结构化数据存储服务。

## 2.Data Model

Bigtable可以简单的理解为一个 稀疏、分布式、持久、多版本的有序Map。该Map通过row key,column key,timestamp进行索引，值为任意的byte数组。

>（row key,column key,timestamp） => string

###2.1 Rows

Row key为任意字符串，并按字典顺序进行排序。在单一row key下的读或写操作都是原子的。

对于一个表，Bigtable根据row key的范围进行分片，每片叫***`tablet`***，这是进行负载均衡和任务分配的基本单位（类似于GFS中的Block，但该分片为逻辑意义）。因此，同一范围的数据将存储在相近的位置，小范围数据的读取也将非常有效，因为只需要和很少的机器通信。

应用也可以利用该特性来提高应用性能，Google内部的Webtable就是一个例子：Webtable存储各种网页的内容和元数据，其将row key设置为该网页网址的逆序（和java包命名规则一样），使得url相近的web内容存储在相近的位置。

###2.2 Column Families

Column family是指将相关的数据组成一组，使得在分布式存储的情况下，可以更高效的访问数据。Bigtable中，Column family为最基本的访问控制单元。

传统情况下，数据以表形式进行存储，应用的任何数据访问都将读取表中的完整行，即包含读取行的所有列信息。对于Bigtable应用场景中，每个表所包含的列数量将比较巨大，而应用读取数据绝大部分情况下只是对其中的某一部分感兴趣，若读取所用行数据则效率低下。通过Column Family方式，将列按数据的逻辑属性进行分类（分类由应用完成），不同分类的数据分别存储，使得在读取的时候，可以仅仅读取部分列数据提高效率。

Bigtable中，Column family中的数据通常为同一属性，并且必须在插入数据前创建，通过如下语法创建：*family:qualifier*。


###2.3 Bigtable vs. Relational DataModels

<figure><img src="/images/bigtable/datamodel.png"/></figure>

传统关系数据库模型中，学生选课关系通过三个表进行存储，表之间通过主键与外键建立联系。
而在Bigtable模型中，分别以学生ID和课程ID建立两个表进行存储。可以看到学生有两个Column Family：info和course。如此，访问学生个人信息只需要读取info这Column Family即可。

##3.Data Store

Bigtable数据存储构建在GFS上，在GFS上的物理块单位为***`SSTable`***。SSTable提供了持久的、有序不可变的key到value的映射。SSTable比较大，在读取和查找数据上效率低下，所以其内部有分为一系列的block（一般为64k大小）。在每个SSTable末尾存储该数据块的索引，当打开该块时，索引将被加载到内存中，使得数据查找只需要一次磁盘访问：首先通过二分查找在内存索引中找到合适的块，然后从磁盘读取该块。并且，SSTable可以完全的映射到内存（一般100~200MB），使得可以高效的执行lookup或scan操作。

Bigtable本身是一个share-nothing架构，对于系统关键性的数据，它将其存储到Chubby中。Chubby是一个分布式、高可用、持久化分布式锁服务，有五个active replicas组成，使用Paxos算法。Bigtable使用Chubby完成如下任务：

- 1 保证任何时刻最多有一个active master
- 2.存储Bigtable数据的引导位置
- 3.发现tablet server以及停止tablet server
- 4.存储Bigtable模式信息（各表的column family信息）
- 5.存储访问控制列表

##4.Implementation

Bigtable主要由三部分组成：1.一个Master server，多个Tablet Server以及一个连接所有客户端的字典。

Master负责分配tablet给Tablet Server,检测Tablet Server的增加和超时，对Tablet Server进行负载均衡，以及Bigtable在GFS上的垃圾数据回收。

Tablet Server管理具体的tablet，处理数据的读写以及当tablet过大时，进行分割。

与GFS不同，Client不需要访问Master就可以直接读取数据。

###4.1 Tablet Location


<figure><img src="/images/bigtable/tabletlocation.png"/></figure>

Bigtable的数据存储使用三级结构。首先是存储在Chubby中的`root tablet`的位置信息，然后是`root tablet`，它指定了二级的元数据信息，二级元数据信息才真正连接到实际的数据中。

在METADATA中，存储了各tablet的位置，每个tablet一条，包含器所属table的标识符以及其最后一行数据的row key。每一条占1kb。若假设METADATA tablets最大为128MB，在该三级结构最大支持2^{34}个tablet.

Client将缓存tablet位置信息。若Client不知道tablet的位置或者缓存的位置信息不正确，它将会递归的寻找tablet的位置。

###4.2 Tablet Assignment

Bigtable中，Master或Tablet Server都可能崩溃，为了实现高可用性，系统的关键元数据都存储在Chubby中。

任何Tablet在任何时间最多被分配给一个Tablet Server，Master追踪Tablet Server状态以及各个Tablet Server当前的tablet分配情况。当发现有未分配的tablet且存在有足够空间分配tablet的Tablet Server时，Master将通过请求将该tablet分配出去。

Bigtable使用Chubby来追踪当前集群状况。当Tablet Server启动时，它将在Chubby一个特定的目录下创建一个唯一的文件，并对此文件上互斥锁。Master通过检测该目录下文件的加锁状况即可知道当前集群各Tablet Server的运行状况。Tablet Server发现其失去互斥锁时将停止服务，若其发现锁文件还存在，则会尝试对其重新加锁，若文件不存在，则表示Master要求其关闭，它将停止服务并退出。

Master需要检测各Tablet Server的运行状态，以了解集群中tablet分配情况。Master会周期性的与所有TabletServer通信，若某个Tablet server报告其无法获取互斥锁或Master无法与其建立联系，Master将尝试获取该Tablet Server文件的互斥锁，以检测Chubby是否故障。若Master能够成功获取互斥锁，则可以判定为Tablet Server故障，将删除其文件，将该Tablet Server排除出去。

Master本身也可能出现故障，比如网络故障、自身bug或者机器崩溃等。Master检测到其在Chubby上的Session过期时，将杀死自己并退出。但Master退出不会影响当前Tablet的分配情况。Master启动时，需要获取集群信息，tablet情况。集群信息可以通过Chubby中互斥锁文件目录获取，tablet分配信息就需要扫描METADATA获得所有的tablet，并通过与tablet Server通信，获取tablet的分配情况。Master启动有如下四个步骤：

* 1.在Chubby中获取一个唯一的*master*互斥锁，保证集群最多只有一个Master
* 2.扫描Chubby中互斥锁目录，获取当前集群的Tablet Server状况
* 3.与各Tablet Server通信，以获取tablet的分配情况
* 4.扫描METADATA，获得集群所有的tablet

若METADATA tablet尚未分配，则上面的第四步就无法完成。此时，就需要将root tablet加入为分配tablet集合中，以便在合适的时候进行分配。

Tablet集合的变化之后出现在四种情况下：创建、删除、合并、分割。前三种操作都是由master发起，最后一个是由tablet server发起，故前三种Master肯定能感知到tablet集合的变化。对于tablet的分割，处理情况比较特殊。分割操作的提交需要在METADATA表中记录新tablet的信息，并通知master。

###4.3 Tablet Serving

<figure><img src="/images/bigtable/tabletrepresentation.png"/></figure>

Bigtable中的所有更新操作都将记录在redo日志中，用以处理Tablet Server故障时的数据恢复。最近的更新将存储在内存中，称为***`memtable`***，较老的更新记录在SSTable文件中。数据恢复时，tablet server将从METADATA中获得该tablet包含的SSTable列表以及操作日志，对日志进行重做以恢复数据。

对于写操作，Tablet Server首先检查tablet是否正常，然后检查权限。检查通过之后，将首先提价操作日志，然后将写入值插入memtable中。为了提高性能，日志提交采用组提交方式（牺牲少量的数据一致性，提高性能）。

对于读操作，需要在由多个SSTable以及memtable的合并视图上进行，由于memtable和SSTable是有序的，合并操作能够高效的完成。

由于SSTable是不可变的，对于更新操作就无法像传统数据库一样直接修改对应的值，而只能在SSTable中记录新插入的值，在读取的时候就需要完整的读取对应的所有值，并获取最终的值。

###4.4 Compaction

Memtable大小达到某个阈值时将进行一次***`minor compaction`***，将当前memtable冻结，并创建一个新的memtable，然后将之前的memtable转换为SSTable存储到GFS中。Minor Compaction有两个作用：1.减少Tablet Server的内存占用；2.减少数据恢复的时间。

每次Minor Compaction都将创建一个新的SSTable，若不加限制，读操作就需要读取任意多的SSTable数据以得到合并的视图。并且，无效的数据也无法进行删除，释放资源。

为了解决上述问题，Bigtable还在后台维护了叫***`major compaction`***的服务。Major Compaction读取数个SSTable以及memtable，去除无效的数据，并输出一个新的SSTable。该操作完成之后，之前的SSTable就可以丢弃。

No-major合并使得内存中的数据存储到磁盘中，减少内存占用，但是其输出的SSTable中，将包含无效的，被删除的数据。而Major合并操作，生成的SSTable将不包含陈旧无效的数据。Major Compaction使得可以回收无效数据占用的资源。

## 5.Refinements

Bigtable采取了一些优化策略，以提高系统性能。

###5.1 Locality Groups

将一般不一起访问的Column Family数据存储到不同的文件中，提高数据读取速度。并支持基于内存的数据存储（例：METADATA tablet），使得数据量较少的情况下，能够高效访问。

###5.2 Compression

Bigtable允许用户自定义的数据压缩。对SSTable进行压缩时，按block为单位进行。这样做会牺牲一些压缩空间效益，但是在数据读取时，不需要对整个SSTable数据进行解压，能得到很好的读取性能。

BigTable中对数据压缩算法的选取更趋向于压缩速度，而不是压缩比。

###5.3 Caching for read performance

Bigtable有两种缓存：1.Scan Cache缓存key-value对；2.Block Cache缓存从SSTable读取的block。

###5.4 Bloom filters

Bloom filters作用是判断一个值是否属于一个集合中。但是它不会直接返回是或者否，而是返回“绝对不存在”或者“可能存在”。

对于读操作，需要进行磁盘访问以构建合并视图。若访问的SSTable不存在，则之前的磁盘访问都是徒劳。通过Bloom filter可以在磁盘访问之前判断SSTable是否存在，以极大的减少不必要的磁盘访问。

###5.5 Commit-log implementation

大量的Tablet Server需要提交日志，若采用全局单一日志文件，并发将很差。若采取每个SSTable一个日志文件，又会使得组提交优化策略效果不佳，因为会使得每个组都很小。所以采取每个Tablet Server一个日志文件的方式，将不同tablet的操作日志混合记录在同一个日志文件中。

该方案使得这场操作效率高，但是恢复时效率低下，行为多个Tablet Server都需要完整拷贝该Tablet Server所有的日志。所以，在恢复时，首先对redo日志进行排序，然后各恢复节点只需要拷贝其中的一部分即可。



